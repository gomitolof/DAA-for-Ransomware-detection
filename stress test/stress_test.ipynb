{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT ALL NEEDED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE STRUCTURAL METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the entropy of a vector\n",
    "# INPUTS\n",
    "# - v: vector that contains in each cell the value of a byte (so from 0 to 255)\n",
    "# - num: number of cells of the vector we want to consider in the analysis (from the first element)\n",
    "# OUTPUT: entropy value\n",
    "def entropy(v, num):\n",
    "    freq = np.zeros(256)\n",
    "    for i in range(num):\n",
    "        freq[int(v[i])]=freq[int(v[i])]+1\n",
    "    h = 0\n",
    "    for i in range(256):\n",
    "        if freq[i]>0:\n",
    "            h = h + (freq[i]/num) * np.log2(freq[i]/num)\n",
    "    h = -h\n",
    "    return h\n",
    "\n",
    "# Function to evaluate the entropy of a file as a function of the header lenght\n",
    "# INPUTS\n",
    "# - file: the file we want to anlyze (formats as an integer vector)\n",
    "# - Bytes: total number of bytes we want to consider in the analysis\n",
    "# OUTPUT: vector that contains the entropy values as a function of the header lenght analyzed\n",
    "\n",
    "def entropy_analysis(file, Bytes):\n",
    "    h_vector = np.zeros(int(Bytes/8)+1)\n",
    "    for i in range(int(Bytes/8)+1):\n",
    "        h_vector[i] = entropy(file, 8*i)\n",
    "    return h_vector\n",
    "\n",
    "# Function to evaluate the Area (DDA approach) between two files \n",
    "# INPUTS\n",
    "# - h_1,h_2: vectors obtained with entropy_analysis function\n",
    "# OUTPUT: value of the area\n",
    "def diff_area(h_1,h_2):\n",
    "    area = 0\n",
    "    length = min(len(h_1),len(h_2))\n",
    "    diff_vec = np.zeros(length)\n",
    "    for i in range(length):\n",
    "        diff_vec[i]=abs(h_1[i]-h_2[i])\n",
    "    sum = 0\n",
    "    for i in range(2,length-2):\n",
    "        sum = sum+2*diff_vec[i]\n",
    "    area = (8/2)*(diff_vec[1]+diff_vec[len(h_1)-1]+sum)\n",
    "    return area\n",
    "\n",
    "# Function to transform the file of the dataset in a vector ready to be analyzed\n",
    "# INPUTS\n",
    "# - file: the file from the dataset we want to analyze\n",
    "# - size: the size of the file we want to cut\n",
    "# OUTPUT: file vector\n",
    "def file_to_vector(file, start, offset):\n",
    "    vec = np.zeros(offset)\n",
    "    for i in range(offset):\n",
    "        vec[i] = int.from_bytes(bytes(file[start + i], 'latin-1'), byteorder=\"big\")\n",
    "    return vec\n",
    "\n",
    "# Function that compute if the file is classified as ransomware\n",
    "# INPUTS\n",
    "# - treshold: value to classify the file\n",
    "# - area: trapezoidal area of the file\n",
    "# OUTPUT: accur (correctly classified) and err (not)\n",
    "def accuracy(threshold, area):\n",
    "    accur = 0.0\n",
    "    #True positive\n",
    "    if area <= threshold:\n",
    "        accur = 1.0\n",
    "    return accur\n",
    "\n",
    "# Function that compute the differential trapezoidal area between ideal and real file\n",
    "# INPUTS\n",
    "# - ideal_file: file of 256 byte completely random\n",
    "# - file_vector: vectorialized version of the file\n",
    "# - best_hl: best header length to classify to compute the entropy, founded in best_model.ipynb\n",
    "# OUTPUT: differential area between ideal and consider file\n",
    "def trapezoidal_rule(ideal_file, file_vector, best_hl):\n",
    "    h_ideal = entropy_analysis(ideal_file, best_hl)\n",
    "    h = entropy_analysis(file_vector, best_hl)\n",
    "    return diff_area(h_ideal,h)\n",
    "\n",
    "# Function that compute the trapezoidal area of the file\n",
    "# INPUTS\n",
    "# - file: self explenatory\n",
    "# - offset: interval of byte in which compute the area\n",
    "# - best_hl: best header length to classify to compute the entropy, founded in best_model.ipynb\n",
    "# - num_frams: parameter that set the different type of analysis\n",
    "# OUTPUT: area of consider file\n",
    "def get_areas(file, offset, best_hl, num_fragms):\n",
    "    start = 0\n",
    "    i = 1\n",
    "    areas = []\n",
    "    bound = len(file) // (len(file) // offset)\n",
    "    while i <= num_fragms and len(file) >= i*offset:\n",
    "        file_vector = file\n",
    "        areas.append(trapezoidal_rule(ideal_file, file_vector, best_hl))\n",
    "        if len(file) >= (i+1)*offset and len(file) - (i+1) * bound + offset < (i+1) * bound + offset:\n",
    "            start = random.randint(i * bound, len(file) - offset)\n",
    "        elif len(file) >= (i+1)*offset and i < num_fragms-1:\n",
    "            start = random.randint(i * bound, (i+1) * bound)\n",
    "        i+=1\n",
    "    return areas\n",
    "\n",
    "# Function that update the accuracies and error statistics\n",
    "# INPUTS\n",
    "# - best_tresh: best threshold for the considere analysis, computed in best_model.ipynb\n",
    "# - sequences: length sequence value for which we are computing accuracy, used as and index for the acc matrix\n",
    "# - jumps: jump value for which we are computing accuracy, used as and index for the acc matrix\n",
    "# - acc: matrix of accuracies, also known as true positive (correctly classified ransomwares)\n",
    "# - false_negatives: matrix of ransomwares incorrecly classified as safe files\n",
    "# OUTPUT: void, just update the matrices acc and false_negatives\n",
    "def update_statistics(best_thresh, sequences, jumps, acc, best_area):\n",
    "    result = accuracy(best_thresh, best_area)\n",
    "    acc[sequences][jumps] = np.add(acc[sequences][jumps], result, dtype=float)\n",
    "\n",
    "# Function that lower the entropy of a ransomware file\n",
    "# INPUTS\n",
    "# - length_sequence: length of the random bytes sequence to insert into the file\n",
    "# - jump: distance between each injection of random bytes\n",
    "# - file: file to modify\n",
    "# OUTPUT: modified file with the requested length sequence and jump\n",
    "def modify_file(length_sequence, jump, file):\n",
    "    modified_file = []\n",
    "    for i in range(0,len(file),jump):\n",
    "        random_value = float(random.randint(0, 255))\n",
    "        random_values = [random_value for i in range(length_sequence)]\n",
    "        modified_file += list(file[i:i+jump]) + random_values\n",
    "    return modified_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATE IDEAL FILE, READ FILES PATH AND INITIALYZE SEQUENCE LENGTHS AND JUMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 256 #interval to analyze\n",
    "ideal_file = np.random.randint(0,offset,offset) #ideal random file for comparison reason\n",
    "\n",
    "# Obtains all paths to analyze from a .txt files\n",
    "f = open('../paths/path_ransomwares.txt', 'r')\n",
    "paths = f.readlines()\n",
    "f.close()\n",
    "\n",
    "sequence_start, sequence_end, sequence_step = 2,59,8 #sequence values to start, end, and step between each number\n",
    "length_sequences = np.arange(sequence_start,sequence_end,sequence_step) #generate array of such values, last values is sequence_end - 1\n",
    "jump_start, jump_end, jump_step = 2,59,8 #jump values to start, end, and step between each number\n",
    "jumps = np.arange(jump_start,jump_end,jump_step) #generate array of such values, last values is jump_end - 1\n",
    "#power = 5 #number of elements in jumps + 1, used for plot porpuses\n",
    "#jumps = np.array([12,24,32,48]) #array of jump length values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRESS TEST FOR AVG AREA\n",
    "DEFINE AVG AREA METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that get the best area between different fragments considered\n",
    "# INPUTS\n",
    "# - distance: parameter that allows the method to mitigate the Davies effect\n",
    "# - offset: interval of byte in which compute the area\n",
    "# - areas: previously computed areas from which we pick the best one\n",
    "# - num_frams: parameter that set the different type of analysis\n",
    "# - file: considered file\n",
    "# OUTPUT: area that better perform for the consider analysis\n",
    "def get_best_area(distance, offset, areas, num_fragms, file):\n",
    "    best_area = 0\n",
    "    i=2\n",
    "    mean_random_area = areas[1]\n",
    "    while i < num_fragms and len(file) >= (i+1)*offset:\n",
    "        mean_random_area = mean_random_area + areas[i]\n",
    "        i+=1\n",
    "    mean_random_area = mean_random_area / (i-1)\n",
    "    if areas[0] - distance < mean_random_area:\n",
    "        best_area = areas[0]\n",
    "    else:\n",
    "        best_area = min(areas[0], mean_random_area)\n",
    "    return best_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENT AND TEST BEST 3F_AVG MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_analyzed = 0 #total files analyzed\n",
    "num_fragms = 3 #type of analysis (3 for 3F, 4 for 4F)\n",
    "\n",
    "#initialyze results matrix\n",
    "acc = np.zeros([len(length_sequences), len(jumps)], dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the best threshold, distance and header length values previously computed\n",
    "thresholds = np.arange(2,35,1)\n",
    "distances = np.arange(24,66,3)\n",
    "acc_des=np.load('../results/acc_3F_mix_avg.npy')\n",
    "acc_des = acc_des.reshape(len(thresholds), len(distances), int(offset/8))\n",
    "\n",
    "ind = np.unravel_index(np.argmax(acc_des, axis=None), acc_des.shape)\n",
    "best_thresh, best_dist, best_hl = thresholds[ind[0]], distances[ind[1]], 8*(ind[2]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the analysis\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over the different directories containing files to analyze\n",
    "for j in range(len(paths)):\n",
    "    # List all the files in the paths[j] directory\n",
    "    files=os.listdir(paths[j][:-1])\n",
    "    print(\"Start analysizng the directory\", paths[j][:-1])\n",
    "    # Iterate over the different files in the directory paths[j]\n",
    "    for object in files:\n",
    "        full_path = paths[j][:-1] + \"/\" + object\n",
    "        f=open(full_path, \"r\", encoding='latin-1')\n",
    "        file = f.read()\n",
    "        file = file_to_vector(file,0,len(file))\n",
    "        for len_seq in range(len(length_sequences)):\n",
    "            for jump in range(len(jumps)):\n",
    "                modified_file = modify_file(length_sequences[len_seq],jumps[jump],file)\n",
    "                if len(modified_file) >= 2*offset:\n",
    "                    areas = get_areas(modified_file, offset, best_hl, num_fragms)\n",
    "                    best_area = get_best_area(best_dist, offset, areas, num_fragms, modified_file)\n",
    "                    update_statistics(best_thresh, len_seq,jump, acc, best_area)\n",
    "                    if(round(acc[len_seq][jump]) <= 0):\n",
    "                        break\n",
    "        files_analyzed += 1\n",
    "        f.close()\n",
    "\n",
    "acc = np.multiply(np.true_divide(acc, float(files_analyzed)), 100.0, dtype=float)\n",
    "\n",
    "time_spent = time.time() - start_time\n",
    "print(\"Time for the analysis of the dataset\")\n",
    "print(\"--- %s minutes, %s seconds, %s files analyzed ---\" % (time_spent//60, ((time_spent)-60*((time_spent)//60))//1, files_analyzed*len(length_sequences)*len(jumps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results\n",
    "acc = np.transpose(acc) #transpose for graphical reasons\n",
    "\n",
    "n = round((jump_end-jump_start)/jump_step) #number of curves to plot = number of jumps\n",
    "#n = power - 1 #number of curves to plot = number of jumps\n",
    "color = cm.rainbow(np.linspace(0,1,n))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"3F_avg\")\n",
    "plt.xlabel(\"Sequence lengths\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "for i,c in zip(range(n),color):\n",
    "    label = \"Jump length \" + str(jumps[i])\n",
    "    plt.plot(length_sequences, acc[i], c=c, label=label)\n",
    "plt.legend()\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results in a file\n",
    "np.savetxt('accuracies_3F_avg_4.txt',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENT AND TEST BEST 4F_AVG MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_analyzed = 0 #total files analyzed\n",
    "num_fragms = 4 #type of analysis (3 for 3F, 4 for 4F)\n",
    "\n",
    "#initialyze results matrix\n",
    "acc = np.zeros([len(length_sequences), len(jumps)], dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the best threshold, distance and header length values previously computed\n",
    "thresholds = np.arange(2,35,1)\n",
    "distances = np.arange(33,66,3)\n",
    "acc_des=np.load('../results/acc_4F_mix_avg.npy')\n",
    "acc_des = acc_des.reshape(len(thresholds), len(distances), int(offset/8))\n",
    "\n",
    "ind = np.unravel_index(np.argmax(acc_des, axis=None), acc_des.shape)\n",
    "best_thresh, best_dist, best_hl = thresholds[ind[0]], distances[ind[1]], 8*(ind[2]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the analysis\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over the different directories containing files to analyze\n",
    "for j in range(len(paths)):\n",
    "    # List all the files in the paths[j] directory\n",
    "    files=os.listdir(paths[j][:-1])\n",
    "    print(\"Start analysizng the directory\", paths[j][:-1])\n",
    "    # Iterate over the different files in the directory paths[j]\n",
    "    for object in files:\n",
    "        full_path = paths[j][:-1] + \"/\" + object\n",
    "        f=open(full_path, \"r\", encoding='latin-1')\n",
    "        file = f.read()\n",
    "        file = file_to_vector(file,0,len(file))\n",
    "        for len_seq in range(len(length_sequences)):\n",
    "            for jump in range(len(jumps)):\n",
    "                modified_file = modify_file(length_sequences[len_seq],jumps[jump],file)\n",
    "                if len(modified_file) >= 2*offset:\n",
    "                    areas = get_areas(modified_file, offset, best_hl, num_fragms)\n",
    "                    best_area = get_best_area(best_dist, offset, areas, num_fragms, modified_file)\n",
    "                    update_statistics(best_thresh, len_seq,jump, acc, best_area)\n",
    "                    if(round(acc[len_seq][jump]) <= 0):\n",
    "                        break\n",
    "        files_analyzed += 1\n",
    "        f.close()\n",
    "\n",
    "acc = np.multiply(np.true_divide(acc, float(files_analyzed)), 100.0, dtype=float)\n",
    "\n",
    "time_spent = time.time() - start_time\n",
    "print(\"Time for the analysis of the dataset\")\n",
    "print(\"--- %s minutes, %s seconds, %s files analyzed ---\" % (time_spent//60, ((time_spent)-60*((time_spent)//60))//1, files_analyzed*len(length_sequences)*len(jumps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results\n",
    "acc = np.transpose(acc) #transpose for graphical reasons\n",
    "\n",
    "n = round((jump_end-jump_start)/jump_step) #number of curves to plot = number of jumps\n",
    "#n = power - 1 #number of curves to plot = number of jumps\n",
    "color = cm.rainbow(np.linspace(0,1,n))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"4F_avg\")\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "for i,c in zip(range(n),color):\n",
    "    label = \"Jump length \" + str(jumps[i])\n",
    "    plt.plot(length_sequences, acc[i], c=c, label=label)\n",
    "plt.legend()\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results in a file\n",
    "np.savetxt('accuracies_4F_avg_4.txt',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRESS TEST FOR MAX AREA\n",
    "DEFINE MAX AREA METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that get the best area between different fragments considered\n",
    "# INPUTS\n",
    "# - distance: parameter that allows the method to mitigate the Davies effect\n",
    "# - offset: interval of byte in which compute the area\n",
    "# - areas: previously computed areas from which we pick the best one\n",
    "# - num_frams: parameter that set the different type of analysis\n",
    "# - file: considered file\n",
    "# OUTPUT: area that better perfomr for the consider analysis\n",
    "def get_best_area(distance, offset, areas, num_fragms, file):\n",
    "    best_area = 0\n",
    "    i=2\n",
    "    rand_Fs_best_area = areas[1]\n",
    "    while i < num_fragms and len(file) >= (i+1)*offset:\n",
    "        rand_Fs_best_area = max(areas[i-1], areas[i])\n",
    "        i+=1\n",
    "    if areas[0] - distance < rand_Fs_best_area:\n",
    "        best_area = areas[0]\n",
    "    else:\n",
    "        best_area = min(areas[0], rand_Fs_best_area)\n",
    "    return best_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENT AND TEST BEST 3F_MAX MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_analyzed = 0 #total files analyzed\n",
    "num_fragms = 3 #type of analysis (3 for 3F, 4 for 4F)\n",
    "\n",
    "#initialyze results matrix\n",
    "acc = np.zeros([len(length_sequences), len(jumps)], dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the best threshold, distance and header length values previously computed\n",
    "thresholds = np.arange(2,35,1)\n",
    "distances = np.arange(24,51,3)\n",
    "acc_des=np.load('../results/acc_3F_mix.npy')\n",
    "acc_des = acc_des.reshape(len(thresholds), len(distances), int(offset/8))\n",
    "\n",
    "ind = np.unravel_index(np.argmax(acc_des, axis=None), acc_des.shape)\n",
    "best_thresh, best_dist, best_hl = thresholds[ind[0]], distances[ind[1]], 8*(ind[2]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the analysis\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over the different directories containing files to analyze\n",
    "for j in range(len(paths)):\n",
    "    # List all the files in the paths[j] directory\n",
    "    files=os.listdir(paths[j][:-1])\n",
    "    print(\"Start analysizng the directory\", paths[j][:-1])\n",
    "    # Iterate over the different files in the directory paths[j]\n",
    "    for object in files:\n",
    "        full_path = paths[j][:-1] + \"/\" + object\n",
    "        f=open(full_path, \"r\", encoding='latin-1')\n",
    "        file = f.read()\n",
    "        file = file_to_vector(file,0,len(file))\n",
    "        for len_seq in range(len(length_sequences)):\n",
    "            for jump in range(len(jumps)):\n",
    "                modified_file = modify_file(length_sequences[len_seq],jumps[jump],file)\n",
    "                if len(modified_file) >= 2*offset:\n",
    "                    areas = get_areas(modified_file, offset, best_hl, num_fragms)\n",
    "                    best_area = get_best_area(best_dist, offset, areas, num_fragms, modified_file)\n",
    "                    update_statistics(best_thresh, len_seq,jump, acc, best_area)\n",
    "                    if(round(acc[len_seq][jump]) <= 0):\n",
    "                        break\n",
    "        files_analyzed += 1\n",
    "        f.close()\n",
    "\n",
    "acc = np.multiply(np.true_divide(acc, float(files_analyzed)), 100.0, dtype=float)\n",
    "\n",
    "time_spent = time.time() - start_time\n",
    "print(\"Time for the analysis of the dataset\")\n",
    "print(\"--- %s minutes, %s seconds, %s files analyzed ---\" % (time_spent//60, ((time_spent)-60*((time_spent)//60))//1, files_analyzed*len(length_sequences)*len(jumps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results\n",
    "acc = np.transpose(acc) #transpose for graphical reasons\n",
    "\n",
    "n = round((jump_end-jump_start)/jump_step) #number of curves to plot = number of jumps\n",
    "#n = power - 1 #number of curves to plot = number of jumps\n",
    "color = cm.rainbow(np.linspace(0,1,n))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"3F_max\")\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "for i,c in zip(range(n),color):\n",
    "    label = \"Jump length \" + str(jumps[i])\n",
    "    plt.plot(length_sequences, acc[i], c=c, label=label)\n",
    "plt.legend()\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results in a file\n",
    "np.savetxt('accuracies_3F_max_4.txt',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENT AND TEST BEST 4F_MAX MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_analyzed = 0 #total files analyzed\n",
    "num_fragms = 4 #type of analysis (3 for 3F, 4 for 4F)\n",
    "\n",
    "#initialyze results matrices\n",
    "acc = np.zeros([len(length_sequences), len(jumps)], dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the best threshold, distance and header length values previously computed\n",
    "thresholds = np.arange(2,35,1)\n",
    "distances = np.arange(27,60,3)\n",
    "acc_des=np.load('../results/acc_4F_mix.npy')\n",
    "acc_des = acc_des.reshape(len(thresholds), len(distances), int(offset/8))\n",
    "\n",
    "ind = np.unravel_index(np.argmax(acc_des, axis=None), acc_des.shape)\n",
    "best_thresh, best_dist, best_hl = thresholds[ind[0]], distances[ind[1]], 8*(ind[2]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the analysis\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over the different directories containing files to analyze\n",
    "for j in range(len(paths)):\n",
    "    # List all the files in the paths[j] directory\n",
    "    files=os.listdir(paths[j][:-1])\n",
    "    print(\"Start analysizng the directory\", paths[j][:-1])\n",
    "    # Iterate over the different files in the directory paths[j]\n",
    "    for object in files:\n",
    "        full_path = paths[j][:-1] + \"/\" + object\n",
    "        f=open(full_path, \"r\", encoding='latin-1')\n",
    "        file = f.read()\n",
    "        file = file_to_vector(file,0,len(file))\n",
    "        for len_seq in range(len(length_sequences)):\n",
    "            for jump in range(len(jumps)):\n",
    "                modified_file = modify_file(length_sequences[len_seq],jumps[jump],file)\n",
    "                if len(modified_file) >= 2*offset:\n",
    "                    areas = get_areas(modified_file, offset, best_hl, num_fragms)\n",
    "                    best_area = get_best_area(best_dist, offset, areas, num_fragms, modified_file)\n",
    "                    update_statistics(best_thresh, len_seq,jump, acc, best_area)\n",
    "                    if(round(acc[len_seq][jump]) <= 0):\n",
    "                        break\n",
    "        files_analyzed += 1\n",
    "        f.close()\n",
    "\n",
    "acc = np.multiply(np.true_divide(acc, float(files_analyzed)), 100.0, dtype=float)\n",
    "\n",
    "time_spent = time.time() - start_time\n",
    "print(\"Time for the analysis of the dataset\")\n",
    "print(\"--- %s minutes, %s seconds, %s files analyzed ---\" % (time_spent//60, ((time_spent)-60*((time_spent)//60))//1, files_analyzed*len(length_sequences)*len(jumps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results\n",
    "acc = np.transpose(acc) #transpose for graphical reasons\n",
    "\n",
    "n = round((jump_end-jump_start)/jump_step) #number of curves to plot = number of jumps\n",
    "#n = power - 1 #number of curves to plot = number of jumps\n",
    "color = cm.rainbow(np.linspace(0,1,n))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"4F_max\")\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "for i,c in zip(range(n),color):\n",
    "    label = \"Jump length \" + str(jumps[i])\n",
    "    plt.plot(length_sequences, acc[i], c=c, label=label)\n",
    "plt.legend()\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results in a file\n",
    "np.savetxt('accuracies_4F_max_4.txt',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3829f2c1eece7dd9fdaa5b0dbec02962a23d51361b4e4655008413bd4e44011"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
